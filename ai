[1편]
-	인공지능은 로봇이다?
	인공지능 = 로봇 이란 공식이 항상 성립하지는 않는다. 인공지능이란 하나의 소프트웨어 프로그램에 지나지 않는다. 로봇은 그걸 담는 용기이다.
-	인공지능은 스스로 똑똑 해질 수 있다?
	인공지능의 90% 이상은 지도학습 방식으로 훈련된다.
	지도학습 방식이란 사람이 데이터 한 건 한 건을 어떻게 판단하거나 처리하는지에 대한 정답을 인공지능에게 알려주면서 진행하는 방식이다.
-	인공지능도 감정이 있다?
	인공지능이 감정을 가질 일은 없다. 다만 감정을 가지는 척하도록 프로그래밍 할 수는 있다.
-	강 인공지능
	인간의 명령이 없어도 스스로 판단하고 결정을 내린다. 범용성을 가지는 인공지능으로 범 인공지능이라고 부른다.
-	약 인공지능
	제한된 환경에서 구체적인 특정 업무를 수행하는 데 있어서 사람과 비슷한, 또는 사람 이상의 성능을 낼 수 있는 인공지능. (ex. 알파고, IBM 암 진단 인공지능)
-	범용성과 전문성의 구별이 절대적이지 않다.
	범용성과 전문성의 구별은 절대적인 게 아니라 정도의 차이이다. 범용적이라고 해서 무엇이든지 다 할 수 있는 것도 아니고, 전문적이라고 해서 단 하나의 작업만 할 수 있는 것도 아니다.
>> AI > ML > DL

-	인공지능: 사람이 해야 할 일을 기계가 대신할 수 있는 모든 자동화에 해당 (넓은 의미의 인공지능에 해당). 특징추출과 판단을 사람이 한다.
>> 규칙 기반 프로그래밍

-	머신러닝: 명시적으로 규칙을 프로그래밍하지 않고 데이터로부터 의사결정을 위한 패턴을 기계가 스스로 학습. 특징추출을 사람이 하고, 기계가 판단한다.
>> 판단모델은 수학적 모델링과 기계학습을 통해 구현

-	딥러닝: 인공신경망 기반의 모델로, 비정형 데이터로부터 특징 추출 및 판단까지 기계가 한번에 수행 (좁은 의미의 인공지능에 해당). 특징추출과 판단을 모두 기계가 한다.
>> 인공신경망학습
 
[2편]
>> ML/DL 기계학습의 한 분야, 비정형데이터를 입력받은 후 데이터의 주요한 특징을 스스로 추출, 의사결정 하는 기술

-	머신러닝이 다루는 정형 데이터
	사람이 정제하고 정리한 데이터 (데이터베이스, CSV, 엑셀 파일 등). 안정성은 높으나 체계적으로 구조가 고정되어 있어 유연하지 않은 데이터타입.
-	딥러닝이 다루는 비정형 데이터 >> (웹페이지, 상품 리뷰, SNS글, 기업용문서, 뉴스기사 등)
	일상생활에서 우리가 마주하는, 조금 더 원자료(Raw Data) 형태에 가까운 데이터.
-	모라벡의 역설
	사람에게 쉬운 것이 기계에게는 어렵다. 기계에게 쉬운 것은 사람에게 어렵다.
-	사람처럼 사고하기
	사례들로부터 말로는 할 수 없는, 사전적 정의가 아닌 특징을 배운다.
	사람이 뉴런으로 데이터를 받아들이고 처리하고 전달하듯 인공적으로 뉴런을 구성하고 여러 층 엮어 데이터를 흘린다면 기계도 사람처럼 사고할 수 있지 않을까?

-	인공뉴런
	생물학적 뉴런의 모양을 그대로 본 따서 만든 것이 ‘인공 뉴런’으로, 인공뉴런은 이전의 뉴런이 넘겨준 데이터를 받아들여 가중합 연산을 한 뒤, 
비선형함수를 적용하여 정보를 가공하여 다음에 이어지는 인공 뉴런으로 데이터를 넘긴다.
-	인공신경망
	이런 인공 뉴런을 다양한 방식으로 여러 층 쌓아 연결하게 되면 딥러닝의 기본 구조인 ‘인공신경망(Artificial Neural Network)’이 된다.
-	딥러닝이 유행하게 된 배경
	양질의 데이터를 다량 확보할 수 있게 되었다.
	높은 스펙의 하드웨어를 확보할 수 있게 되었다.
 
[3편]
-	기계가 이미지를 인식하는 방법
>> 인공뉴런으로 구성된 인공신경망이 가중합연산, 벡터/행렬의 형태
	사람이 보는 시각 정보를 인공지능은 계산 가능한 데이터로 바꾸어 인식한다. 이렇게 입력된 데이터는 인공신경망 연산이 가능해진다. 
(이미지를 2차원 행렬로 변환, 컬러 이미지를 3차원 텐서 RGB로 변환)
-	CNN(convolution neural network)
>>	이미지 처리에 특화된 신경망. 특징을 추출하는 Feature Extraction 영역과 
특정 태스크 수행을 위해 덧붙이는 태스크 수행영역 두 가지로 구성되어있다.

-	Feature Extraction
	이미지로부터 특징을 추출하는 역할은 컨볼루션(Convolution) 연산과 풀링(Pooling) 연산이 수행. 
이미지로부터 특징을 배워나가는 작업이라는 뜻에서 이 과정을 'Feature Learning'이라고 부른다.
	컨볼루션 연산은 컨볼루션 필터(또는 커널)가 입력 이미지를 상하좌우로 훑으며 주요한 특징이 있는지 찾아내는 과정. 
이렇게 찾아낸 결과 특징을 Feature map(또는 convolved feature)라고 부른다.
	풀링 연산은 Feature map을 역시 상하좌우로 훑으며 핵심적인 정보만을 영역별로 샘플링한다. 
>> 컨볼루션 연산이 이미지의 특징을 찾아낸다면 풀링 연산은 그 중 핵심정보만 남긴다.
>> Maxpooling 방식사용(영역내에 가장큰값만 찾고 나머진 버림)

-	태스크 수행
	Classification. 입력으로 받은 이미지를 지정된 K개의 클래스(또는 카테고리) 중 하나로 분류하는 과제 
(강아지와 고양이를 분류하거나, 공장에서 제품 사진을 보고 양호/불량을 판별하는 업무 등)
	Detection. 입력으로 받은 이미지에서 특정 개체가 어디에 위치하는지, (x, y) 좌표값을 찾아주는 과제 
(스마트폰으로 사진을 찍을 때 자동으로 인물의 얼굴에 네모박스를 치고 포커스하는 기능)
	Segmentation. Detection이 네모 박스로 개체를 찾아준다면 Segmentation은 조금 더 정밀하게, 
픽셀 단위로 영역을 구별해준다. 경계를 찾는 작업
-	ImageNet 데이터 분류 대회
	이 대회에 출전하는 인공지능 모델은 입력 받은 ImageNet 데이터를 무려 1,000개 카테고리 중 하나로 분류해야 하는 과제를 수행한다.
어떤 인공지능이 더 이미지를 잘 인식하는지에 대한 기준은 Top-5 에러율로 정한다. 
2012년, 토론토 대학의 Alex Krizhevsky라는 연구자가 처음으로 딥러닝 모델을 사용한 AlexNet으로 오류율을 크게 개선시킨다.
 
[4편]
>> 자연어처리(NLP) > 자연어이해(NLU)
-	자연어이해(NLU)
	자연어란 사람들이 일상적으로 쓰는 언어 (한국어, 영어, 중국어 등) <-> 인공어란 의도와 목적에 따라 인공적으로 만들어진 언어 (프로그래밍 언어, 에스페란토 등)
	언어 데이터는 많은 정보를 굉장히 함축하여 인코딩한 자료이다. 기계가 언어를 인식하고 맥락에 포함된 내용을 이해하기란 쉽지 않다.
	>> 인공지능에게 기대하는 것은 단순 규칙에 따른 처리 (NLP) 가 아닌, 내용과 맥락을 이해 (NLU) 하여 업무를 수행하는 것이다. NLP 안에 NLU가 포함된다.
-	기계에게 사람의 언어를 인식시키려면?
	>> 언어를 다차원의 실수 데이터인 텐서(Tensor)로 만들어 인공신경망에 태울 수 있어야 한다.
>> 어절/     형태소          /음절(voca list 13000개)/자소(voca list 몇백개)
>> X   / 논문,뉴스기사,연설문 / 음성,채팅, 소셜미디어  / 채팅, 소셜미디어
-	Tonkenizing(Parsing) >> 읽을수 있는 토큰(보통 단어단위)
	한 덩이로 되어있는 문장을 인공신경망에 인식시키기 위해서 세부 단위로 쪼개는 작업. 쪼개진 단위를 토큰이라 부른다. 어떻게 쪼개면 좋을지는 언어의 특징과 수행하고자 하는 태스크, 데이터의 특징에 따라 다르다.
-	워드임베딩
	>> 토큰을 벡터화하는 것을 워드임베딩이라고 부른다.
	원-핫 인코딩. 우리가 알고 있는 모든 토큰들을 쭈루룩 줄세워 사전을 만들고, 순서대로 번호를 붙이는 작업. 
>> 토큰이 다양하고 수가 많을수록 토큰 하나를 표현하기 위해서 굉장히 길이가 긴 벡터를 필요로 한다.

	>> CBOW. CBOW 방식은 인공지능에게 문장을 알려주되 중간중간 빈칸을 만들어 들어갈 단어(토큰)을 유추시킨다.
	>> SKIPGRAM. SKIPGRAM 방식은 인공지능에게 단어(토큰) 하나를 알려주고, 주변에 등장할 만한 그럴싸한 문맥을 만들도록 시킨다. 
많은 문장을 학습시키면 시킬수록 더 좋은 품질의 벡터가 나온다.
-	다양한 자연어이해 과제들
	문장/문서 분류 (사용자 리뷰를 감성분석(긍/부정)하거나, 유저의 발화문을 챗봇이 처리할 수 있는 기능 중 하나로 매핑하는 의도분류 등)
	Sequence-to-Sequence. (한 나라의 언어를 다른 나라로 옮기는 번역과제라든지, 긴 문서를 핵심 문장으로 추리는 요약과제)
	질의 응답 (사용자 질문이 들어오면 내가 가진 매뉴얼 내에서 가장 답변이 될 가능성이 높은 영역을 리턴하는 MRC(Machine Reading Comprehension)와, 가장 유사한 과거 질문/답변(FAQ)를 꺼내주는 IR(Information Retrieval) 형태. 상담챗봇 및 콜센터)

 
[5편]
-	시계열 데이터(Sequential data) 예측분석
	순차적인 시계열 데이터를 활용하여 근미래에 어떤 데이터 값이 나타날지를 예측하는 과제
-	순환 신경망(RNN)
	과거에 데이터를 처리하여 결과를 출력했던 과정의 일부를 가져와 현 시점에서 데이터를 처리하고 결과를 출력하는 데 도움을 주는 인공신경망.
-	순환 신경망(RNN) 장점
	RNN은 시간 흐름에 따른 과거 정보를 누적할 수 있다.
	>> RNN은 가변 길이의 데이터를 처리할 수 있다. RNN은 과거의 정보를 매 timestep마다 압축하여 
>> 다음 timestep으로 넘기므로 데이터의 길이에 무관하게 자유롭게 구성할 수 있다.
	RNN은 다양한 구성의 모델을 만들 수 있다. >> 이 때 입력 데이터의 정보를 누적하는 부분을 인코딩(Encoding), 
결과를 출력하는 부분을 디코딩(Decoding)이라고 표현한다.
-	순환 신경망(RNN) 단점
	>> 연산속도가 느리다. 병렬학습이 어렵고, 순차적으로 데이터를 처리해야 하는 성질로 인해 연산 속도가 다소 느리다.
	학습이 불안정하다.
1.	timestep이 길어지면 RNN 인공신경망이 반영해야 할 과거의 이력이 많아지게 된다. 
>> 이 과정에서 인공신경망이 학습해야 할 값이 폭발적으로 증가하는 현상이 발생할 수 있다 (Gradient Exploding).
2.	>> timestep이 길어지면 저 멀리 있는 과거의 이력은 현재의 추론에 거의 영향을 미치지 못하는 문제가 생긴다(Gradient Vanishing)
	실질적으로 과거 정보를 잘 활용할 수 있는 모델이 아니다. RNN은 한 timestep씩 정보를 누적하여 인코딩하는데, 
먼 과거의 정보는 여러 번 압축되고 누적되다 보니 거의 영향을 미치지 못한다 (장기 종속성/의존성 문제 – Long-term dependency)
-	LTSM(Long-short term memory)
	먼 과거의 정보 중 중요한 것은 기억하고, 불필요한 것은 잊어버리도록 스스로 조절 가능한 RNN 유닛. 
Forget Gate (과거의 정보 필터링) / Input Gate (현재의 정보 필터링) / Output Gate (넘길 정보 필터링) 로 구성됨. 연산속도가 좀 더 느려진다.
-	오늘날 인공신경망을 활용하는 대부분의 시계열 예측은 아주 간단한 과제를 제외하고는 기본 RNN을 사용하는 경우는 거의 없다. 
대부분은 LSTM, GRU과 같은 개선된 유닛을 활용하며, 대부분의 딥러닝 프레임워크에서 쉽게 구현할 수 있도록 기능을 제공하고 있다.
>> RNN도 텍스트 데이터에 연결가능
 
[6편]
-	AI 프로세스
	AI를 도입하는 과정은 크게 오프라인 프로세스와 온라인 프로세스로 나뉜다.
-	오프라인 프로세스
	>> AI 모델러는 기 확보된 데이터를 확인하고 정제하여 필요한 부분을 취하거나(Generate features) 필요한 경우 라벨을 붙인다(Collect labels). 
라벨이란 AI 모델 학습을 위해 필요한 정보로, 인공지능이 맞춰야 하는 '정답’을 의미한다.
	데이터를 마련했으면 어떤 머신러닝/딥러닝 알고리즘을 활용하여 모델을 학습할 것인지 정하고, 
>> 좋은 성능을 달성할 때까지(Validate & Select models) 반복실험을 진행한다(Train models). 
이 때 발견한 문제점을 해결하거나 더 개선된 성능을 낼 수 있도록 실험(Experimentation)을 반복하는데, 이 과정을 튜닝이라고 한다.
	여러 실험을 반복하며 개선된 성능의 모델은 배포할 수 있도록 최종 선택(Publish model)된다.
	여기까지의 과정은 더 나은 AI모델을 만들기 위해 모델을 학습시키는 과정으로, Training Pipeline이라고 한다.

-	온라인 프로세스
	모델 추론을 포함한 온라인 프로세스의 대부분은 머신러닝/딥러닝이라기보다는 개발 영역(Application)에 가깝다.
AI 모델을 운영 환경에 띄운다면(Load Model) 모델이 처리할 데이터는 기존에 정리하여 모아 놓은 데이터가 아닌 운영 환경의 스트리밍 데이터(Live Data)이다.
AI 모델은 과거의 학습 지식을 바탕으로 현장의 데이터를 추론하게 된다.
-	일반화 성능(Generalization)
	이전에 본적 없는 데이터에 대해서도 잘 수행하는 능력
-	오버피팅
	>> 훈련시에만 잘 작동하고 일반화 성능이 떨어지는 모델을 오버피팅 되었다고 한다.
-	Training, Validation, Test set
	>> 엄격히 비율이 정해져 있는 것은 아니지만, 보통은 Training, Validation, Test set을 8:1:1, 6:2:2 정도로 구분한다.
-	Training set
	>> Training set은 머신러닝/딥러닝 모델을 학습하는 데 이용하는 데이터이다. 이는 단순 최적화(Optimization)에 해당한다.
-	Validation set
	>> Validation set은 머신러닝/딥러닝 모델에게 정답을 알려 줄 데이터는 아니지만, 우리가 모델을 튜닝하는 데 도움을 주는 데이터이다. 
모델의 일반화 성능을 판단하여 이어질 실험을 계획하는 데 이용한다.
-	Test set
	Test set은 모델의 학습에 어떤 식으로도 전혀 관여하지 않는 데이터로, 오로지 모델의 최종 성능을 평가하기 위해 따로 떼어놓은 데이터이다. 
여러 모델간 성능을 비교할 땐 Test set에 대한 스코어를 활용한다.
-	학습곡선
	학습 곡선이란 학습이 진행됨에 따라 모델의 성능을 기록하는 그래프이다.
	오버피팅이 발생했는지 확인할 수 있는 방법은 
학습 곡선상에서 Training set와 Validation set에 대한 모델의 성능이 어떻게 변화하는지를 확인해보는 것이다.

-	Regularization
	오버피팅을 피하기 위한 모든 전략들을 Regularization이라고 한다. Regularization의 목적은 Generalization, 즉 일반화 성능을 향상하는 데 있다.
-	데이터 증강(Data Augumentation)
	얼마 없는 데이터 건수를 마치 많은 것처럼, 데이터를 증강시키는 방법. 한 장의 이미지를 좌우 반전 시키거나,
일부 영역을 크롭하거나, 노이즈를 추가하거나, 색상, 명암, 채도 등에 변화를 주어 모델 학습에 추가로 데이터를 이용할 수 있다.
-	Capacity 줄이기
	Capacity는 모델의 복잡한 정도를 나타내는 개념이다. 신경망을 여러층 쌓거나 뉴런의 수를 많이 둘 수록 Capacity가 높아진다고 말할 수 있다.
Capacity가 필요 이상으로 너무 높은 모델은 주어진 데이터를 외우게 될 가능성이 높다.
	만일 오버피팅의 경향이 발견된다 싶으면 내 모델의 층 수를 줄여본다든지, 한 층의 뉴런 수를 줄인다든지 등의 조치를 취해볼 수 있다.
-	조기 종료(Early Stopping)
	오버피팅이 감지될 경우 목표하는 학습 시간이 다 되지 않았다고 하더라도 '조기 종료'해버리는 것
-	드롭아웃(Dropout)
	드롭아웃은 학습 과정(Training pipeline)에서 일정 비율 p 만큼의 노드(인공 뉴런)를 무작위로 끄고 진행하는 Regularization 기법이다. 
딥러닝 모델 학습 시 굉장히 많이 적용하는 Regularization 기법 중 하나. 
일부 노드가 사라진 상태에서 남아있는 노드만으로 어떻게든 정답을 맞춰야 하는 인공지능 모델은 훨씬 더 강력해진다

 
[7편]
-	인공지능 적용시의 문제점
	구체적이지 않으며 불명확한 태스크
	적은 데이터, 낮은 품질의 데이터
>> 이미지 텍스트 비정형 데이터는 딥러닝
>> 양질의 다양한 데이터가 필요
	다른 도메인 환경
-	전이 학습(Transfer Learning)
	한 번 만들어진 딥러닝 모델을 재활용하여 쓸 수 있는 기법. 
비슷한 태스크를 다른 도메인에 적용할 때, 그리고 그 태스크를 위한 학습 데이터가 부족한 경우 유용하게 쓰일 수 있다.
-	Catastrophic forgetting: 치명적인 기억상실
	딥러닝 모델이 새로운 정보를 학습할 때 이전에 배웠던 정보를 완전히 잊어 버리는 경향. 
전이(Transfer)가 필요 이상으로 과하게 일어난 경우이다.

>> 더나은 Transfer Learning 방법
-	>> 레이어 동결(Layer Freezing)
	새 태스크에 기 보유 모델을 Transfer Learning 하려 할 땐 기본 특징 학습은 건너 뛰고 바로 태스크를 위한 학습으로 가는 것이 좋다. 
이를 위해 후반부의 신경망 층에 대해서만 파라미터를 학습하고, 전반부의 파라미터는 학습되지 않도록 고정해 놓는 기법을 적용할 수 있다.
-	Discriminative fine-tuning
	층마다 Learning rate(학습률)의 차별을 두는 방법. 
>> Learning rate이란 한 번에 인공신경망의 파라미터를 얼마만큼 업데이트 시킬지에 대한 정도로, >> 사람이 설정하는 값(hyperparameter)이다. 
전반부의 인공신경망은 Learning rate을 작게 설정하고, 후반부의 인공신경망은 Learning rate을 크게 설정하여 빠르게 성큼성큼 학습하는 편이 좋다.
-	전이학습의 활용
	이미지넷을 이용한 컴퓨터 비전에서의 전이학습.
>> TrnasferLearning 자연어이해(NLU) 효과적임
	자사에서 개발한 한국어 표준 질의응답 데이터 KorQuAD를 이용한 자연어이해(NLU) 전이학습.

 
[8편]
-	사전학습 (Pre-training)
	전이 학습을 염두에 두고 다방면으로 활용할 수 있는 모델을 미리 만들어 놓는 것.
	이러한 학습을 Pre-training, 또는 사전학습이라고 하며 이러한 용도의 모델을 Pre-trained Model, 즉 사전학습 모델이라고 한다.
	모델의 사전학습은 대규모의 오픈도메인 데이터에 대해 이뤄지는 것이 일반적이다.
	>> 시각데이터에 대한 사전학습 이미지넷은 가장 유명한 이미지 사전학습 과제라고 할 수 있다.
>> 언어데이터에 대한 사전 학습 위키피디아
	동영상 사전 학습용 데이터: Youtube-8M

>> 대규모로 구할수 있는 데이터라해도 라벨까지 잘 달려있진않다.
-	자가지도학습 (Self-Supervised Learning)
supervised Learning/ semi/ unsupervised learning
>> 인공지능이 입력을 받으면 추론해야하는 데이터 라벨 
	사람이 만들어주는 정답 라벨이 없어도 기계가 시스템적으로 자체 라벨을 만들어서 사용하는 학습 방법. 
주로 사전학습에서 이용되며 다량의 데이터는 있으나 라벨은 없는 경우에 활용
	일반적으로 Self-Supervised Learning을 활용한 
Pre-Trained 모델은 다량의 방대한 지식을 골고루 습득하는 것을 목적으로 하기에 대체로 모델의 사이즈가 큰 편이고 
사전학습 규모가 어마어마하다는 특징이 있다.
	Self-Supervised Learning 기법으로 사전학습을 하고 다양한 태스크에 Transfer Learning을 할 수 있는 
대표적인 예로 Google의 'BERT'라는 모델이 있다. 자연어처리과제에 사용되는 사전학습 모델.

 
[9편]
-	능동학습 (Active Learning)
	Active Learning은 라벨링을 할 수 있는 인적 자원은 있지만, 많은 수의 라벨링을 수행할 수 없을 때 효과적으로 라벨링을 하기 위한 기법이다.
	Active Learning은 학습 데이터 중 모델 성능 향상에 효과적인 데이터들을 선별한 후, 선별한 데이터를 활용해 학습을 진행하는 방법이다.
	반대로 주어진 라벨 데이터만 가지고 모델을 학습하는 방법을 Passive Learning(수동 학습)이라고 한다.
>> 모델이 잘맞추기 어려운 데이터를 찾아 학습한다면, 더 적은 훈련시간으로 더 좋은 성능을 낼수 있다.
>> 라벨링을 위한 예산이 한정되었을때, 모델의 성능을 극대화 할 수 있는 라벨링 데이터를 찾기

-	Active Learning 절차
	Training a Model: 초기 학습 데이터(labeled data)를 이용해 모델을 학습합니다.
	Select Query: 라벨이 되지 않은 데이터 풀로부터 모델에게 도움이 되는 데이터를 선별합니다.
	Human Labeling: 선별한 데이터를 사람이 확인하여 라벨을 태깅합니다.
	선별한 라벨 데이터를 기존 학습 데이터와 병합한 후, 다시 모델을 학습합니다.

-	SelectQuery 단계에서의 쿼리 전략 (Query Strategy)
	성능 향상에 효과적인 데이터를 선별하는 방법
	쿼리전략의 대표적인 두 가지: Uncertainty Sampling, Query by committee
-	Uncertainty Sampling
	AI 모델은 가장 불확실하다(least certain)고 생각하는 데이터를 추출하여 라벨링이 필요하다고 요청한다.
-	Query by committee
	Query by committee는 여러 AI 모델간의 의견불일치를 종합 고려하는 방식이다.
여러 모델간 추론한 결과 불일치가 많은 데이터일수록 가장 헷갈리는 데이터, 즉 라벨링을 진행할 대상이 된다.

 
[10편]
-	어텐션 메커니즘(Attention mechanism)
	중요하다고 판단한 해당 단어에 조금 더 집중하여 전체 입력을 다시 한번 재조정한 입력 데이터 인코딩 벡터로 만드는 방식 
(딥러닝 모델이 스스로 집중할 영역을 파악한다)
-	어텐션 스코어(Attention sore)
	중요한 단어에 집중한다는 것은 어텐션 스코어를 계산한다는 것이다. 
	어텐션 스코어는 인공신경망 모델이 각 인코딩 timestep마다 계산된 특징(feature)를 가지고 자동으로 계산하는 0~1사이의 값이다. 
어떤 step은 더 집중해서 봐야하고(1에 가까운 스코어), 어떤 스텝은 지금은 중요하지 않으므로 대충대충 살피도록(0에 가까운 스코어) 하는 것이다.
-	컨텍스트 벡터(Context vector)
	어텐션 스코어를 구하고 나면 현재 디코딩할 단어와의 관련성을 반영하여 다시 입력 문장을 인코딩하게 되는데 
이는 중요도에 따라 전체 문맥의 정보를 잘 반영하고 있다고 하여 컨텍스트 벡터(Context vector)라고 부른다.
>> 딥러닝 모델이 스스로 집중할 영역을 파악

-	설명가능한 인공지능(XAI)
	딥러닝 기반의 인공지능은 일반 머신러닝 기반이나 전통적 룰 기반의 프로그래밍에 비해 예측 정확도는 좋지만, 
그 모델이 너무 복잡하고 해석하기 어렵다는 단점이 있다.
	어텐션은 인공신경망의 이러한 설명 부족 문제를 일부 해소해줄 수 있다.
	텍스트에서의 어텐션 예시: 소비자 민원 주제 자동분류
	이미지에서의 어텐션 예시: 이미지 캡셔닝(이미지를 입력으로 받아 설명문을 생성)

-	트랜스포머
	어텐션만으로 이루어진 인공신경망 구조
	트랜스포머(Transformer)라는 인공신경망은 입력 데이터끼리의 self-attention을 통해 상호 정보교환을 수행하는 것이 특징.
	순차적 계산이 필요 없기 때문에 RNN보다 빠르면서도 맥락 파악을 잘하고, CNN처럼 일부씩만을 보는 것이 아니고 전 영역을 아우른다.
	하지만 이해력이 좋은 대신에 모델의 크기가 엄청 커지며 고사양의 하드웨어 스펙을 요구한다는 단점이 있다.

 
[11편]
-	>> 자동화된 기계학습(AutoML)
	사람이 주기적으로 실험에 개입하지 않아도 AI 스스로가 반복실험을 통해 성능을 개선하는 것
	AutoML의 세가지 역할: 
>> 데이터로부터 중요한 특징(feature)을 선택하고 인코딩하는 방식에 대한 Feature Engineering 자동화
>> AI 모델 학습에 필요한 사람의 설정들, 하이퍼파라미터를 자동으로 탐색
>> AI 모델의 구조 자체를 더 효율적인 방향으로 찾아주는 아키텍쳐 탐색
-	하이퍼파라미터 탐색 자동화
	그리드 서치방식(Grid Search): 그리드 서치 방식은 최적화할 하이퍼파라미터의 값 구간을 일정 단위로 나눈 후, 
각 단위 조합을 테스트하여 가장 높은 성능을 낸 하이퍼파라미터 조합을 선택하는 방식이다. 
단순하지만 최적화 대상이 되는 하이퍼파라미터가 많다면 경우의 수가 기하급수적으로 많아져서 탐색에 오랜 시간이 걸린다. 
또한 불필요한 탐색에 시간을 허비하기도 한다.
	랜덤 서치방식(Random Search): 랜덤 서치 방식은 랜덤하게 하이퍼파라미터의 조합을 테스트하는 방식인데, 
그리드서치에 비해 비교적 빠르게 최적의 조합을 찾아내곤 한다.
	Meta Learner: 모델을 통한 하이퍼파라미터 탐색 방식. Meta Learner는 대부분 RNN과 강화학습을 활용하여 최적의 하이퍼파라미터를 탐색한다.
이 때 Meta Learner가 수행하는 학습에 대해, (Learner의)학습을 위한 학습이라는 뜻에서 '메타학습', 또는 'Learn to Learn'이라고 표현하기도 한다.
Meta Learner의 하이퍼파라미터 조합대로 학습한 Learner의 학습 성능 결과를 Meta Learner로 다시 전달하고,
Meta Learner는 이를 또 개선하기 위한 다른 하이퍼파라미터 조합을 내며 Learner는 이 조합으로 또다시 학습하고...
이러한 과정을 반복하다 보면 최적의 조합을 찾아낸다는 이론입니다.

-	아키텍처 탐색 자동화 (NAS)
	사람이 어떤 방식으로 모델 구조를 짤지 생각하지 않아도 자동 탐색을 통해 최적 구조를 찾는 방법. 딥러닝 모델의 경우에는 인공신경망을 활용하기 때문에,
NAS(Neural Architecture search)라고 부른다.
	NAS의 경우도 마찬가지로 대부분 Meta Learner와 Learner로 이루어져 있어서,
Learner가 본 과제를 수행하는 AI 모델이라면 Meta Learner가 어떤 구조의 신경망을 만들면 좋은지, 아키텍쳐 구성을 고민하게 된다.
	Meta Learner는 역시 RNN과 강화학습을 접목한 형식으로 구성해볼 수 있다.
-	AutoML의 특징
	AutoML은 일반적으로는 사람이 고민한 모델 이상의 성능을 낼 수 있다.
	기계는 사람이 생각도 못한 조합의 설정이나 구조를 시도해볼 수 있고, 기존의 설정 관습이나 제약에 얽매이지 않기 때문이다.
-	AutoML 서비스
	AWS, Azure, GCP(Google Cloud Platform)와 같은 CSP 업체는 모두 일종의 AutoML 서비스를 제공하고 있다.

 
[12편]
-	설명가능한 인공지능(XAI)
	인공신경망으로 구성된 딥러닝 모델은 알고리즘의 복잡성으로 인해 "블랙박스"라는 별칭으로도 불린다. 
때문에 딥러닝 모델에 설명력을 부여하는 것은 사용자가 모델의 최종 결정을 이해하고, 결과의 타당성을 확인할 수 있게 해준다.
	인공지능에게 설명력을 부여하는 방법에 대한 연구 분야를 XAI(eXplainable AI), 설명 가능한 인공지능이라고 한다.
	XAI는 모델에 설명 가능한 근거와 해석력을 부여해서 투명성, 신뢰성을 확보하고자 하는 것이 목적이다.
-	XAI를 위한 접근 법: 기존 AI 모델에 설명할 수 있는 모듈을 덧붙이는 방식
	어텐션 메커니즘을 활용한 XAI
	설명하는 법 학습하기(Learn to explain): 판단을 하는 딥러닝 모델에 RNN 모듈 등을 덧붙여 인간이 이해할 수 있는 방식의 설명을 생성하도록 하는 방식.
-	XAI를 위한 접근 법: 애초에 설명력 있는 모델을 만드는 방법
	딥러닝 모델에는 적용 불가
-	XAI를 위한 접근 법: 인공신경망처럼 복잡한 블랙박스 모델의 일부분을 설명해 줄 수 있는 다른 모델을 활용하여 유추
	LIME이나 SP-LIME 등의 기술
